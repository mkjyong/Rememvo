<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Welcome to CSTR’s NN-TTS documentation! &mdash; CSTR NN-TTS 0.0.1 documentation</title>
    
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="CSTR NN-TTS 0.0.1 documentation" href="#" />
   
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="welcome-to-cstr-s-nn-tts-documentation">
<h1>Welcome to CSTR&#8217;s NN-TTS documentation!<a class="headerlink" href="#welcome-to-cstr-s-nn-tts-documentation" title="Permalink to this headline">¶</a></h1>
<p>Contents:</p>
<div class="toctree-wrapper compound">
<ul class="simple">
</ul>
</div>
</div>
<div class="section" id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><a class="reference internal" href="genindex.html"><span>Index</span></a></li>
<li><a class="reference internal" href="py-modindex.html"><span>Module Index</span></a></li>
<li><a class="reference internal" href="search.html"><span>Search Page</span></a></li>
</ul>
</div>
<div class="section" id="get-started">
<h1>Get Started<a class="headerlink" href="#get-started" title="Permalink to this headline">¶</a></h1>
<div class="section" id="required-softwares-tools">
<h2>Required softwares/tools<a class="headerlink" href="#required-softwares-tools" title="Permalink to this headline">¶</a></h2>
<dl class="docutils">
<dt>The system has been tested in linux environment, and the following packages are required.</dt>
<dd><ul class="first last simple">
<li>Python 2.6/2.7</li>
<li>Theano 0.6/0.7 (<a class="reference external" href="http://deeplearning.net/software/theano/">http://deeplearning.net/software/theano/</a>). 0.8 version is not tested yet.</li>
<li>Bandmat 0.5: <a class="reference external" href="https://pypi.python.org/pypi/bandmat/0.5">https://pypi.python.org/pypi/bandmat/0.5</a></li>
<li>SPTK: <a class="reference external" href="http://sp-tk.sourceforge.net/">http://sp-tk.sourceforge.net/</a></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="data-preparation-for-a-neural-network-nn-based-speech-synthesis-system">
<h2>Data Preparation for a neural network (NN) based speech synthesis system<a class="headerlink" href="#data-preparation-for-a-neural-network-nn-based-speech-synthesis-system" title="Permalink to this headline">¶</a></h2>
<p>To build a NN system, you need to prepare linguistic features as system input and acoustic features as system output. Please follow the instructions in this section to prepare your data.</p>
<div class="section" id="input-linguistic-features">
<h3>Input Linguistic Features<a class="headerlink" href="#input-linguistic-features" title="Permalink to this headline">¶</a></h3>
<p>Neural networks take vectors as input, so the alphabet representation of linguistic features needs to be vectorized.</p>
<ol class="arabic">
<li><dl class="first docutils">
<dt><strong>HTS style</strong>: Please check the HTS demo for the HTS style labels (<a class="reference external" href="http://hts.sp.nitech.ac.jp/">http://hts.sp.nitech.ac.jp/</a>).</dt>
<dd><ul class="first last simple">
<li>Provide HTS full-context labels with state-level alignments.</li>
<li>Provide a question file that matches the HTS labels.</li>
<li>The questions in the question file will be used to convert the full-context labels into binary and/or numerical features for vectorization. It is suggested to do a manual selection of the questions, as the number of questions will affect the dimensionality of the vectorized input features.</li>
<li>Different from the HTS format question, the NN system also supports to extract numerical values using &#8216;<strong>CQS</strong>&#8216;, e.g., ** CQS &#8220;Pos_C-Word_in_C-Phrase(Fw)&#8221; {:(d+)+}**, where &#8216;<strong>:</strong>&#8216; and &#8216;<strong>+</strong>&#8216; are separators, and &#8216;<strong>(d+)</strong>&#8216; is a regular expression to match a numerical feature.</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first"><strong>&#8220;Composed&#8221; style</strong>:</p>
</li>
<li><dl class="first docutils">
<dt><strong>Direct *vectorized* input</strong>: If you prefer to do vectorization yourself, you can feed the system binary files directly. Please prepare your binary files with the following instructions:</dt>
<dd><ul class="first last simple">
<li>Align the input feature vectors with the acoustic features. Input and output features should have the same number of frames.</li>
<li>Store the data in binary format with &#8216;<strong>float32</strong>&#8216; precision.</li>
<li>In the config file, use an empty question file, and set <em>appended_input_dim</em> to be the dimensionality of the input vector.</li>
<li>Note: voice conversion can use this kind of direct vectorized input.</li>
</ul>
</dd>
</dl>
</li>
</ol>
</div>
<div class="section" id="output-acoustic-features">
<h3>Output Acoustic Features<a class="headerlink" href="#output-acoustic-features" title="Permalink to this headline">¶</a></h3>
<dl class="docutils">
<dt>The default setting is assuming you use the STRAIGHT vocoder (c version). This vocoder is free for academic users. The output includes</dt>
<dd><ul class="first last simple">
<li>mel-cepstral coeffcients (MCC),</li>
<li>band aperiodicities (BAP),</li>
<li>Fundamental frequency (F0) in logarithmic scale.</li>
</ul>
</dd>
<dt>Please provide the three features in binary format with &#8216;float32&#8217; precision, in the config file, provide the dimensionality of each feature, for example</dt>
<dd><ul class="first last simple">
<li>[Outputs] mgc  :  60</li>
<li>[Outputs] dmgc :  180</li>
</ul>
</dd>
</dl>
<p><strong>dmgc</strong> means the dimensionality of MCC with delta and delta delta features. If <strong>dmgc</strong> is set to 60, only the static features are used.
Please also tell the file extension for each feature, for example</p>
<blockquote>
<div><ul class="simple">
<li>[Extensions] mgc_ext : .mgc</li>
<li>[Extensions] bap_ext : .bap</li>
<li>[Extensions] lf0_ext : .lf0</li>
</ul>
</div></blockquote>
<p>The open-source WORLD vocoder is also supported. The modified version for SPSS can be found in the repository.</p>
<p>If you have your preferred vocoder, please try to give a nick name to each feature to match the supported ones.</p>
</div>
</div>
<div class="section" id="recipes">
<h2>Recipes<a class="headerlink" href="#recipes" title="Permalink to this headline">¶</a></h2>
<p>In the system, several recipes for standard neural network architectures are provided. They are described below:</p>
<div class="section" id="architecture">
<h3>Architecture<a class="headerlink" href="#architecture" title="Permalink to this headline">¶</a></h3>
<dl class="docutils">
<dt>The system supports a flexible way to change neural network architectures by changing the config file in the [Architecture] section:</dt>
<dd><ul class="first last simple">
<li>hidden_layer_size  : [512, 512, 512, 512]</li>
<li>hidden_layer_type  : [&#8216;TANH&#8217;, &#8216;TANH&#8217;, &#8216;TANH&#8217;, &#8216;TANH&#8217;]</li>
</ul>
</dd>
<dt>By default, feedforward neural network is used. But the system supports various types of hidden layers:</dt>
<dd><ul class="first last simple">
<li><strong>&#8216;TANH&#8217;</strong> : The Hyperbolic Tangent activation function</li>
<li><strong>&#8216;RNN&#8217;</strong>  : The simple but standard recurrent neural network unit</li>
<li><strong>&#8216;LSTM&#8217;</strong> : The standard Long Short-Term Memory unit</li>
<li><strong>&#8216;GRU&#8217;</strong>  : The gated recurrent unit</li>
<li><strong>&#8216;SLSTM&#8217;</strong>: The simplified LSTM unit</li>
<li><strong>&#8216;BLSTM&#8217;</strong>: The bidirectional LSTM unit</li>
</ul>
</dd>
</dl>
<p>You can define your own architecture by choosing a hidden unit at each hidden layer. For each type of hidden layer, please check the <strong>Models</strong> section.</p>
</div>
<div class="section" id="deep-feedforward-neural-network">
<h3>Deep Feedforward Neural Network<a class="headerlink" href="#deep-feedforward-neural-network" title="Permalink to this headline">¶</a></h3>
<p>An example config file can be found in the &#8216;./recipes/dnn&#8217; directory. Please use &#8216;submit.sh ./run_lstm.py ./recipes/dnn/feed_foward_dnn.conf&#8217; to build the feedforward neural network. Please modify the config file to adapt to your own working environment (e.g., data path).</p>
</div>
<div class="section" id="mixture-density-neural-network">
<h3>Mixture Density Neural Network<a class="headerlink" href="#mixture-density-neural-network" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="deep-long-short-term-memory-lstm-based-recurrent-neural-network-rnn">
<h3>(Deep) Long Short-Term Memory (LSTM) based Recurrent Neural Network (RNN)<a class="headerlink" href="#deep-long-short-term-memory-lstm-based-recurrent-neural-network-rnn" title="Permalink to this headline">¶</a></h3>
<p>An example config file is provided in &#8216;./recipes/dnn/hybrid_lstm.conf&#8217;. Follow the same recipe as that in the deep feedforward neural network section.</p>
</div>
<div class="section" id="deep-hybrid-bidirectional-lstm-based-rnn">
<h3>(Deep/Hybrid) Bidirectional LSTM-based RNN<a class="headerlink" href="#deep-hybrid-bidirectional-lstm-based-rnn" title="Permalink to this headline">¶</a></h3>
<p>Example config files are provided in &#8216;./recipes/blstm&#8217; directory. &#8216;blstm.conf&#8217; is for multiple bidrectional LSTM layers, while &#8216;hybrid_blstm.conf&#8217; is for a hybrid architecture, that uses several feedforward layers at the bottom, and one BLSTM layer at the top.</p>
</div>
</div>
<div class="section" id="variants-of-lstm">
<h2>Variants of LSTM<a class="headerlink" href="#variants-of-lstm" title="Permalink to this headline">¶</a></h2>
<p>This recipe is to support the paper by Wu &amp; King (ICASSP 2016). Several variants of LSTMs are provided. Please use the corresponding config files to do the experiments.</p>
<div class="section" id="stacked-bottlenecks">
<h3>Stacked Bottlenecks<a class="headerlink" href="#stacked-bottlenecks" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="trajectory-modelling">
<h3>Trajectory modelling<a class="headerlink" href="#trajectory-modelling" title="Permalink to this headline">¶</a></h3>
</div>
</div>
</div>
<div class="section" id="models">
<h1>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h1>
<div class="section" id="deep-feedforward-recurrent-networks">
<h2>Deep Feedforward/Recurrent Networks<a class="headerlink" href="#deep-feedforward-recurrent-networks" title="Permalink to this headline">¶</a></h2>
<p>This is something I want to say that is not in the docstring.</p>
<dl class="class">
<dt id="models.deep_rnn.DeepRecurrentNetwork">
<em class="property">class </em><code class="descclassname">models.deep_rnn.</code><code class="descname">DeepRecurrentNetwork</code><span class="sig-paren">(</span><em>n_in</em>, <em>hidden_layer_size</em>, <em>n_out</em>, <em>L1_reg</em>, <em>L2_reg</em>, <em>hidden_layer_type</em>, <em>output_type='LINEAR'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/models/deep_rnn.html#DeepRecurrentNetwork"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#models.deep_rnn.DeepRecurrentNetwork" title="Permalink to this definition">¶</a></dt>
<dd><p>This class is to assemble various neural network architectures. From basic feedforward neural network to bidirectional gated recurrent neural networks and hybrid architecture. <strong>Hybrid</strong> means a combination of feedforward and recurrent architecture.</p>
<dl class="method">
<dt id="models.deep_rnn.DeepRecurrentNetwork.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>n_in</em>, <em>hidden_layer_size</em>, <em>n_out</em>, <em>L1_reg</em>, <em>L2_reg</em>, <em>hidden_layer_type</em>, <em>output_type='LINEAR'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/models/deep_rnn.html#DeepRecurrentNetwork.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#models.deep_rnn.DeepRecurrentNetwork.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>This function initialises a neural network</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_in</strong> &#8211; Dimensionality of input features</li>
<li><strong>hidden_layer_size</strong> (<em>A list of integers</em>) &#8211; The layer size for each hidden layer</li>
<li><strong>n_out</strong> (<em>Integrer</em>) &#8211; Dimensionality of output features</li>
<li><strong>hidden_layer_type</strong> &#8211; the activation types of each hidden layers, e.g., TANH, LSTM, GRU, BLSTM</li>
<li><strong>L1_reg</strong> &#8211; the L1 regulasation weight</li>
<li><strong>L2_reg</strong> &#8211; the L2 regulasation weight</li>
<li><strong>output_type</strong> &#8211; the activation type of the output layer, by default is &#8216;LINEAR&#8217;, linear regression.</li>
<li><strong>p_dropout</strong> &#8211; the dropout rate, a float number between 0 and 1.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="models.deep_rnn.DeepRecurrentNetwork.build_finetune_functions">
<code class="descname">build_finetune_functions</code><span class="sig-paren">(</span><em>train_shared_xy</em>, <em>valid_shared_xy</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/models/deep_rnn.html#DeepRecurrentNetwork.build_finetune_functions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#models.deep_rnn.DeepRecurrentNetwork.build_finetune_functions" title="Permalink to this definition">¶</a></dt>
<dd><p>This function is to build finetune functions and to update gradients</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>train_shared_xy</strong> (<em>tuple of shared variable</em>) &#8211; theano shared variable for input and output training data</li>
<li><strong>valid_shared_xy</strong> (<em>tuple of shared variable</em>) &#8211; theano shared variable for input and output development data</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">finetune functions for training and development</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="models.deep_rnn.DeepRecurrentNetwork.parameter_prediction">
<code class="descname">parameter_prediction</code><span class="sig-paren">(</span><em>test_set_x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/models/deep_rnn.html#DeepRecurrentNetwork.parameter_prediction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#models.deep_rnn.DeepRecurrentNetwork.parameter_prediction" title="Permalink to this definition">¶</a></dt>
<dd><p>This function is to predict</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>test_set_x</strong> (<em>python array variable</em>) &#8211; input features for a testing sentence</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">predicted features</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="layers">
<h1>Layers<a class="headerlink" href="#layers" title="Permalink to this headline">¶</a></h1>
<div class="section" id="recurrent-neural-network-units">
<h2>Recurrent Neural Network units<a class="headerlink" href="#recurrent-neural-network-units" title="Permalink to this headline">¶</a></h2>
<p>This is something I want to say that is not in the docstring.</p>
<dl class="class">
<dt id="layers.gating.VanillaRNN">
<em class="property">class </em><code class="descclassname">layers.gating.</code><code class="descname">VanillaRNN</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#VanillaRNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.VanillaRNN" title="Permalink to this definition">¶</a></dt>
<dd><p>This class implements a standard recurrent neural network: h_{t} = f(W^{hx}x_{t} + W^{hh}h_{t-1}+b_{h})</p>
<dl class="method">
<dt id="layers.gating.VanillaRNN.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#VanillaRNN.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.VanillaRNN.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>This is to initialise a standard RNN hidden unit</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>rng</strong> &#8211; random state, fixed value for randome state for reproducible objective results</li>
<li><strong>x</strong> &#8211; input data to current layer</li>
<li><strong>n_in</strong> &#8211; dimension of input data</li>
<li><strong>n_h</strong> &#8211; number of hidden units/blocks</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="layers.gating.VanillaRNN.recurrent_as_activation_function">
<code class="descname">recurrent_as_activation_function</code><span class="sig-paren">(</span><em>Wix</em>, <em>h_tm1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#VanillaRNN.recurrent_as_activation_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.VanillaRNN.recurrent_as_activation_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement the recurrent unit as an activation function. This function is called by self.__init__().</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Wix</strong> (<em>matrix</em>) &#8211; it equals to W^{hx}x_{t}, as it does not relate with recurrent, pre-calculate the value for fast computation</li>
<li><strong>h_tm1</strong> (<em>matrix, each row means a hidden activation vector of a time step</em>) &#8211; contains the hidden activation from previous time step</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">h_t is the hidden activation of current time step</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="layers.gating.LstmBase">
<em class="property">class </em><code class="descclassname">layers.gating.</code><code class="descname">LstmBase</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#LstmBase"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.LstmBase" title="Permalink to this definition">¶</a></dt>
<dd><p>This class provides as a base for all long short-term memory (LSTM) related classes. 
Several variants of LSTM were investigated in (Wu &amp; King, ICASSP 2016): Zhizheng Wu, Simon King, &#8220;Investigating gated recurrent neural networks for speech synthesis&#8221;, ICASSP 2016</p>
<dl class="method">
<dt id="layers.gating.LstmBase.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#LstmBase.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.LstmBase.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialise all the components in a LSTM block, including input gate, output gate, forget gate, peephole connections</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>rng</strong> &#8211; random state, fixed value for randome state for reproducible objective results</li>
<li><strong>x</strong> &#8211; input to a network</li>
<li><strong>n_in</strong> (<em>integer</em>) &#8211; number of input features</li>
<li><strong>n_h</strong> (<em>integer</em>) &#8211; number of hidden units</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="layers.gating.LstmBase.lstm_as_activation_function">
<code class="descname">lstm_as_activation_function</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#LstmBase.lstm_as_activation_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.LstmBase.lstm_as_activation_function" title="Permalink to this definition">¶</a></dt>
<dd><p>A genetic recurrent activation function for variants of LSTM architectures.
The function is called by self.recurrent_fn().</p>
</dd></dl>

<dl class="method">
<dt id="layers.gating.LstmBase.recurrent_fn">
<code class="descname">recurrent_fn</code><span class="sig-paren">(</span><em>Wix</em>, <em>Wfx</em>, <em>Wcx</em>, <em>Wox</em>, <em>h_tm1</em>, <em>c_tm1=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#LstmBase.recurrent_fn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.LstmBase.recurrent_fn" title="Permalink to this definition">¶</a></dt>
<dd><p>This implements a genetic recurrent function, called by self.__init__().</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Wix</strong> &#8211; pre-computed matrix applying the weight matrix W on  the input units, for input gate</li>
<li><strong>Wfx</strong> &#8211; Similar to Wix, but for forget gate</li>
<li><strong>Wcx</strong> &#8211; Similar to Wix, but for cell memory</li>
<li><strong>Wox</strong> &#8211; Similar to Wox, but for output gate</li>
<li><strong>h_tm1</strong> &#8211; hidden activation from previous time step</li>
<li><strong>c_tm1</strong> &#8211; activation from cell memory from previous time step</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">h_t is the hidden activation of current time step, and c_t is the activation for cell memory of current time step</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="layers.gating.VanillaLstm">
<em class="property">class </em><code class="descclassname">layers.gating.</code><code class="descname">VanillaLstm</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#VanillaLstm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.VanillaLstm" title="Permalink to this definition">¶</a></dt>
<dd><p>This class implements the standard LSTM block, inheriting the genetic class <a class="reference internal" href="#layers.gating.LstmBase" title="layers.gating.LstmBase"><code class="xref py py-class docutils literal"><span class="pre">layers.gating.LstmBase</span></code></a>.</p>
<dl class="method">
<dt id="layers.gating.VanillaLstm.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#VanillaLstm.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.VanillaLstm.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialise a vanilla LSTM block</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>rng</strong> &#8211; random state, fixed value for randome state for reproducible objective results</li>
<li><strong>x</strong> &#8211; input to a network</li>
<li><strong>n_in</strong> (<em>integer</em>) &#8211; number of input features</li>
<li><strong>n_h</strong> (<em>integer</em>) &#8211; number of hidden units</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="layers.gating.VanillaLstm.lstm_as_activation_function">
<code class="descname">lstm_as_activation_function</code><span class="sig-paren">(</span><em>Wix</em>, <em>Wfx</em>, <em>Wcx</em>, <em>Wox</em>, <em>h_tm1</em>, <em>c_tm1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#VanillaLstm.lstm_as_activation_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.VanillaLstm.lstm_as_activation_function" title="Permalink to this definition">¶</a></dt>
<dd><p>This function treats the LSTM block as an activation function, and implements the standard LSTM activation function.
The meaning of each input and output parameters can be found in <a class="reference internal" href="#layers.gating.LstmBase.recurrent_fn" title="layers.gating.LstmBase.recurrent_fn"><code class="xref py py-func docutils literal"><span class="pre">layers.gating.LstmBase.recurrent_fn()</span></code></a></p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="layers.gating.LstmNFG">
<em class="property">class </em><code class="descclassname">layers.gating.</code><code class="descname">LstmNFG</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#LstmNFG"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.LstmNFG" title="Permalink to this definition">¶</a></dt>
<dd><p>This class implements a LSTM block without the forget gate, inheriting the genetic class <a class="reference internal" href="#layers.gating.LstmBase" title="layers.gating.LstmBase"><code class="xref py py-class docutils literal"><span class="pre">layers.gating.LstmBase</span></code></a>.</p>
<dl class="method">
<dt id="layers.gating.LstmNFG.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#LstmNFG.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.LstmNFG.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialise a LSTM with the forget gate</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>rng</strong> &#8211; random state, fixed value for randome state for reproducible objective results</li>
<li><strong>x</strong> &#8211; input to a network</li>
<li><strong>n_in</strong> (<em>integer</em>) &#8211; number of input features</li>
<li><strong>n_h</strong> (<em>integer</em>) &#8211; number of hidden units</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="layers.gating.LstmNFG.lstm_as_activation_function">
<code class="descname">lstm_as_activation_function</code><span class="sig-paren">(</span><em>Wix</em>, <em>Wfx</em>, <em>Wcx</em>, <em>Wox</em>, <em>h_tm1</em>, <em>c_tm1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#LstmNFG.lstm_as_activation_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.LstmNFG.lstm_as_activation_function" title="Permalink to this definition">¶</a></dt>
<dd><p>This function treats the LSTM block as an activation function, and implements the LSTM (without the forget gate) activation function.
The meaning of each input and output parameters can be found in <a class="reference internal" href="#layers.gating.LstmBase.recurrent_fn" title="layers.gating.LstmBase.recurrent_fn"><code class="xref py py-func docutils literal"><span class="pre">layers.gating.LstmBase.recurrent_fn()</span></code></a></p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="layers.gating.LstmNIG">
<em class="property">class </em><code class="descclassname">layers.gating.</code><code class="descname">LstmNIG</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#LstmNIG"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.LstmNIG" title="Permalink to this definition">¶</a></dt>
<dd><p>This class implements a LSTM block without the input gate, inheriting the genetic class <a class="reference internal" href="#layers.gating.LstmBase" title="layers.gating.LstmBase"><code class="xref py py-class docutils literal"><span class="pre">layers.gating.LstmBase</span></code></a>.</p>
<dl class="method">
<dt id="layers.gating.LstmNIG.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#LstmNIG.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.LstmNIG.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialise a LSTM with the input gate</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>rng</strong> &#8211; random state, fixed value for randome state for reproducible objective results</li>
<li><strong>x</strong> &#8211; input to a network</li>
<li><strong>n_in</strong> (<em>integer</em>) &#8211; number of input features</li>
<li><strong>n_h</strong> (<em>integer</em>) &#8211; number of hidden units</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="layers.gating.LstmNIG.lstm_as_activation_function">
<code class="descname">lstm_as_activation_function</code><span class="sig-paren">(</span><em>Wix</em>, <em>Wfx</em>, <em>Wcx</em>, <em>Wox</em>, <em>h_tm1</em>, <em>c_tm1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#LstmNIG.lstm_as_activation_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.LstmNIG.lstm_as_activation_function" title="Permalink to this definition">¶</a></dt>
<dd><p>This function treats the LSTM block as an activation function, and implements the LSTM (without the input gate) activation function.
The meaning of each input and output parameters can be found in <a class="reference internal" href="#layers.gating.LstmBase.recurrent_fn" title="layers.gating.LstmBase.recurrent_fn"><code class="xref py py-func docutils literal"><span class="pre">layers.gating.LstmBase.recurrent_fn()</span></code></a></p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="layers.gating.LstmNOG">
<em class="property">class </em><code class="descclassname">layers.gating.</code><code class="descname">LstmNOG</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#LstmNOG"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.LstmNOG" title="Permalink to this definition">¶</a></dt>
<dd><p>This class implements a LSTM block without the output gate, inheriting the genetic class <a class="reference internal" href="#layers.gating.LstmBase" title="layers.gating.LstmBase"><code class="xref py py-class docutils literal"><span class="pre">layers.gating.LstmBase</span></code></a>.</p>
<dl class="method">
<dt id="layers.gating.LstmNOG.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#LstmNOG.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.LstmNOG.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialise a LSTM with the output gate</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>rng</strong> &#8211; random state, fixed value for randome state for reproducible objective results</li>
<li><strong>x</strong> &#8211; input to a network</li>
<li><strong>n_in</strong> (<em>integer</em>) &#8211; number of input features</li>
<li><strong>n_h</strong> (<em>integer</em>) &#8211; number of hidden units</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="layers.gating.LstmNOG.lstm_as_activation_function">
<code class="descname">lstm_as_activation_function</code><span class="sig-paren">(</span><em>Wix</em>, <em>Wfx</em>, <em>Wcx</em>, <em>Wox</em>, <em>h_tm1</em>, <em>c_tm1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#LstmNOG.lstm_as_activation_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.LstmNOG.lstm_as_activation_function" title="Permalink to this definition">¶</a></dt>
<dd><p>This function treats the LSTM block as an activation function, and implements the LSTM (without the output gate) activation function.
The meaning of each input and output parameters can be found in <a class="reference internal" href="#layers.gating.LstmBase.recurrent_fn" title="layers.gating.LstmBase.recurrent_fn"><code class="xref py py-func docutils literal"><span class="pre">layers.gating.LstmBase.recurrent_fn()</span></code></a></p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="layers.gating.LstmNoPeepholes">
<em class="property">class </em><code class="descclassname">layers.gating.</code><code class="descname">LstmNoPeepholes</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#LstmNoPeepholes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.LstmNoPeepholes" title="Permalink to this definition">¶</a></dt>
<dd><p>This class implements a LSTM block without the peephole connections, inheriting the genetic class <a class="reference internal" href="#layers.gating.LstmBase" title="layers.gating.LstmBase"><code class="xref py py-class docutils literal"><span class="pre">layers.gating.LstmBase</span></code></a>.</p>
<dl class="method">
<dt id="layers.gating.LstmNoPeepholes.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#LstmNoPeepholes.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.LstmNoPeepholes.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialise a LSTM with the peephole connections</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>rng</strong> &#8211; random state, fixed value for randome state for reproducible objective results</li>
<li><strong>x</strong> &#8211; input to a network</li>
<li><strong>n_in</strong> (<em>integer</em>) &#8211; number of input features</li>
<li><strong>n_h</strong> (<em>integer</em>) &#8211; number of hidden units</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="layers.gating.LstmNoPeepholes.lstm_as_activation_function">
<code class="descname">lstm_as_activation_function</code><span class="sig-paren">(</span><em>Wix</em>, <em>Wfx</em>, <em>Wcx</em>, <em>Wox</em>, <em>h_tm1</em>, <em>c_tm1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#LstmNoPeepholes.lstm_as_activation_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.LstmNoPeepholes.lstm_as_activation_function" title="Permalink to this definition">¶</a></dt>
<dd><p>This function treats the LSTM block as an activation function, and implements the LSTM (without the output gate) activation function.
The meaning of each input and output parameters can be found in <a class="reference internal" href="#layers.gating.LstmBase.recurrent_fn" title="layers.gating.LstmBase.recurrent_fn"><code class="xref py py-func docutils literal"><span class="pre">layers.gating.LstmBase.recurrent_fn()</span></code></a></p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="layers.gating.SimplifiedLstm">
<em class="property">class </em><code class="descclassname">layers.gating.</code><code class="descname">SimplifiedLstm</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#SimplifiedLstm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.SimplifiedLstm" title="Permalink to this definition">¶</a></dt>
<dd><p>This class implements a simplified LSTM block which only keeps the forget gate, inheriting the genetic class <a class="reference internal" href="#layers.gating.LstmBase" title="layers.gating.LstmBase"><code class="xref py py-class docutils literal"><span class="pre">layers.gating.LstmBase</span></code></a>.</p>
<dl class="method">
<dt id="layers.gating.SimplifiedLstm.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#SimplifiedLstm.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.SimplifiedLstm.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialise a LSTM with the peephole connections</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>rng</strong> &#8211; random state, fixed value for randome state for reproducible objective results</li>
<li><strong>x</strong> &#8211; input to a network</li>
<li><strong>n_in</strong> (<em>integer</em>) &#8211; number of input features</li>
<li><strong>n_h</strong> (<em>integer</em>) &#8211; number of hidden units</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="layers.gating.SimplifiedLstm.lstm_as_activation_function">
<code class="descname">lstm_as_activation_function</code><span class="sig-paren">(</span><em>Wix</em>, <em>Wfx</em>, <em>Wcx</em>, <em>Wox</em>, <em>h_tm1</em>, <em>c_tm1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#SimplifiedLstm.lstm_as_activation_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.SimplifiedLstm.lstm_as_activation_function" title="Permalink to this definition">¶</a></dt>
<dd><p>This function treats the LSTM block as an activation function, and implements the LSTM (simplified LSTM) activation function.
The meaning of each input and output parameters can be found in <a class="reference internal" href="#layers.gating.LstmBase.recurrent_fn" title="layers.gating.LstmBase.recurrent_fn"><code class="xref py py-func docutils literal"><span class="pre">layers.gating.LstmBase.recurrent_fn()</span></code></a></p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="layers.gating.GatedRecurrentUnit">
<em class="property">class </em><code class="descclassname">layers.gating.</code><code class="descname">GatedRecurrentUnit</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#GatedRecurrentUnit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.GatedRecurrentUnit" title="Permalink to this definition">¶</a></dt>
<dd><p>This class implements a gated recurrent unit (GRU), as proposed in Cho et al 2014 (<a class="reference external" href="http://arxiv.org/pdf/1406.1078.pdf">http://arxiv.org/pdf/1406.1078.pdf</a>).</p>
<dl class="method">
<dt id="layers.gating.GatedRecurrentUnit.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>rng</em>, <em>x</em>, <em>n_in</em>, <em>n_h</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/layers/gating.html#GatedRecurrentUnit.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#layers.gating.GatedRecurrentUnit.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialise a gated recurrent unit</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>rng</strong> &#8211; random state, fixed value for randome state for reproducible objective results</li>
<li><strong>x</strong> &#8211; input to a network</li>
<li><strong>n_in</strong> (<em>integer</em>) &#8211; number of input features</li>
<li><strong>n_h</strong> (<em>integer</em>) &#8211; number of hidden units</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="i-o-functions">
<h1>I/O functions<a class="headerlink" href="#i-o-functions" title="Permalink to this headline">¶</a></h1>
<div class="section" id="binary-i-o-collections">
<h2>Binary I/O collections<a class="headerlink" href="#binary-i-o-collections" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="io_funcs.binary_io.BinaryIOCollection">
<em class="property">class </em><code class="descclassname">io_funcs.binary_io.</code><code class="descname">BinaryIOCollection</code><a class="reference internal" href="_modules/io_funcs/binary_io.html#BinaryIOCollection"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#io_funcs.binary_io.BinaryIOCollection" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
</div>
<div class="section" id="utils">
<h1>Utils<a class="headerlink" href="#utils" title="Permalink to this headline">¶</a></h1>
<div class="section" id="data-provider">
<h2>Data Provider<a class="headerlink" href="#data-provider" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="utils.providers.ListDataProvider">
<em class="property">class </em><code class="descclassname">utils.providers.</code><code class="descname">ListDataProvider</code><span class="sig-paren">(</span><em>x_file_list</em>, <em>y_file_list</em>, <em>n_ins=0</em>, <em>n_outs=0</em>, <em>buffer_size=500000</em>, <em>sequential=False</em>, <em>shuffle=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/utils/providers.html#ListDataProvider"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#utils.providers.ListDataProvider" title="Permalink to this definition">¶</a></dt>
<dd><p>This class provides an interface to load data into CPU/GPU memory utterance by utterance or block by block.</p>
<p>In speech synthesis, usually we are not able to load all the training data/evaluation data into RAMs, we will do the following three steps:</p>
<ul class="simple">
<li>Step 1: a data provide will load part of the data into a buffer</li>
<li>Step 2: training a DNN by using the data from the buffer</li>
<li>Step 3: Iterate step 1 and 2 until all the data are used for DNN training. Until now, one epoch of DNN training is finished.</li>
</ul>
<p>The utterance-by-utterance data loading will be useful when sequential training is used, while block-by-block loading will be used when the order of frames is not important.</p>
<p>This provide assumes binary format with float32 precision without any header (e.g. HTK header).</p>
<dl class="method">
<dt id="utils.providers.ListDataProvider.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>x_file_list</em>, <em>y_file_list</em>, <em>n_ins=0</em>, <em>n_outs=0</em>, <em>buffer_size=500000</em>, <em>sequential=False</em>, <em>shuffle=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/utils/providers.html#ListDataProvider.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#utils.providers.ListDataProvider.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialise a data provider</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>x_file_list</strong> (<em>python list</em>) &#8211; list of file names for the input files to DNN</li>
<li><strong>y_file_list</strong> &#8211; list of files for the output files to DNN</li>
<li><strong>n_ins</strong> &#8211; the dimensionality for input feature</li>
<li><strong>n_outs</strong> &#8211; the dimensionality for output features</li>
<li><strong>buffer_size</strong> &#8211; the size of the buffer, indicating the number of frames in the buffer. The value depends on the memory size of RAM/GPU.</li>
<li><strong>shuffle</strong> &#8211; True/False. To indicate whether the file list will be shuffled. When loading data block by block, the data in the buffer will be shuffle no matter this value is True or False.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="utils.providers.ListDataProvider.load_next_partition">
<code class="descname">load_next_partition</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/utils/providers.html#ListDataProvider.load_next_partition"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#utils.providers.ListDataProvider.load_next_partition" title="Permalink to this definition">¶</a></dt>
<dd><p>Load one block data. The number of frames will be the buffer size set during intialisation.</p>
</dd></dl>

<dl class="method">
<dt id="utils.providers.ListDataProvider.load_next_utterance">
<code class="descname">load_next_utterance</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/utils/providers.html#ListDataProvider.load_next_utterance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#utils.providers.ListDataProvider.load_next_utterance" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the data for one utterance. This function will be called when utterance-by-utterance loading is required (e.g., sequential training).</p>
</dd></dl>

<dl class="method">
<dt id="utils.providers.ListDataProvider.make_shared">
<code class="descname">make_shared</code><span class="sig-paren">(</span><em>data_set</em>, <em>data_name</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/utils/providers.html#ListDataProvider.make_shared"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#utils.providers.ListDataProvider.make_shared" title="Permalink to this definition">¶</a></dt>
<dd><p>To make data shared for theano implementation. If you want to know why we make it shared, please refer the theano documentation: <a class="reference external" href="http://deeplearning.net/software/theano/library/compile/shared.html">http://deeplearning.net/software/theano/library/compile/shared.html</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>data_set</strong> &#8211; normal data in CPU memory</li>
<li><strong>data_name</strong> &#8211; indicate the name of the data (e.g., &#8216;x&#8217;, &#8216;y&#8217;, etc)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">shared dataset &#8211; data_set</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="utils.providers.ListDataProvider.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/utils/providers.html#ListDataProvider.reset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#utils.providers.ListDataProvider.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>When all the files in the file list have been used for DNN training, reset the data provider to start a new epoch.</p>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="front-end">
<h1>Front-end<a class="headerlink" href="#front-end" title="Permalink to this headline">¶</a></h1>
<div class="section" id="label-normalisation">
<h2>Label normalisation<a class="headerlink" href="#label-normalisation" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="frontend.label_normalisation.HTSLabelNormalisation">
<em class="property">class </em><code class="descclassname">frontend.label_normalisation.</code><code class="descname">HTSLabelNormalisation</code><span class="sig-paren">(</span><em>question_file_name=None</em>, <em>subphone_feats='full'</em>, <em>continuous_flag=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/frontend/label_normalisation.html#HTSLabelNormalisation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#frontend.label_normalisation.HTSLabelNormalisation" title="Permalink to this definition">¶</a></dt>
<dd><p>This class is to convert HTS format labels into continous or binary values, and store as binary format with float32 precision.</p>
<dl class="docutils">
<dt>The class supports two kinds of questions: QS and CQS.</dt>
<dd><p class="first"><strong>QS</strong>: is the same as that used in HTS</p>
<p class="last"><strong>CQS</strong>: is the new defined question in the system.  Here is an example of the question: CQS C-Syl-Tone {_(d+)+}. regular expression is used for continous values.</p>
</dd>
</dl>
<p>Time alignments are expected in the HTS labels. Here is an example of the HTS labels:</p>
<p>3050000 3100000 xx~#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&amp;1-4#1-3$1-4&gt;0-1&lt;0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&amp;1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&amp;L-L%/I/0_0/J/4+3-1[2]</p>
<p>3100000 3150000 xx~#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&amp;1-4#1-3$1-4&gt;0-1&lt;0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&amp;1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&amp;L-L%/I/0_0/J/4+3-1[3]</p>
<p>3150000 3250000 xx~#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&amp;1-4#1-3$1-4&gt;0-1&lt;0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&amp;1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&amp;L-L%/I/0_0/J/4+3-1[4]</p>
<p>3250000 3350000 xx~#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&amp;1-4#1-3$1-4&gt;0-1&lt;0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&amp;1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&amp;L-L%/I/0_0/J/4+3-1[5]</p>
<p>3350000 3900000 xx~#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&amp;1-4#1-3$1-4&gt;0-1&lt;0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&amp;1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&amp;L-L%/I/0_0/J/4+3-1[6]</p>
<p>305000 310000 are the starting and ending time.
[2], [3], [4], [5], [6] mean the HMM state index.</p>
<dl class="method">
<dt id="frontend.label_normalisation.HTSLabelNormalisation.wildcards2regex">
<code class="descname">wildcards2regex</code><span class="sig-paren">(</span><em>question</em>, <em>convert_number_pattern=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/frontend/label_normalisation.html#HTSLabelNormalisation.wildcards2regex"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#frontend.label_normalisation.HTSLabelNormalisation.wildcards2regex" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert HTK-style question into regular expression for searching labels.
If convert_number_pattern, keep the following sequences unescaped for 
extracting continuous values):</p>
<blockquote>
<div>(d+)       &#8211; handles digit without decimal point
([d.]+)   &#8211; handles digits with and without decimal point</div></blockquote>
</dd></dl>

</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="#">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Welcome to CSTR&#8217;s NN-TTS documentation!</a></li>
<li><a class="reference internal" href="#indices-and-tables">Indices and tables</a></li>
<li><a class="reference internal" href="#get-started">Get Started</a><ul>
<li><a class="reference internal" href="#required-softwares-tools">Required softwares/tools</a></li>
<li><a class="reference internal" href="#data-preparation-for-a-neural-network-nn-based-speech-synthesis-system">Data Preparation for a neural network (NN) based speech synthesis system</a><ul>
<li><a class="reference internal" href="#input-linguistic-features">Input Linguistic Features</a></li>
<li><a class="reference internal" href="#output-acoustic-features">Output Acoustic Features</a></li>
</ul>
</li>
<li><a class="reference internal" href="#recipes">Recipes</a><ul>
<li><a class="reference internal" href="#architecture">Architecture</a></li>
<li><a class="reference internal" href="#deep-feedforward-neural-network">Deep Feedforward Neural Network</a></li>
<li><a class="reference internal" href="#mixture-density-neural-network">Mixture Density Neural Network</a></li>
<li><a class="reference internal" href="#deep-long-short-term-memory-lstm-based-recurrent-neural-network-rnn">(Deep) Long Short-Term Memory (LSTM) based Recurrent Neural Network (RNN)</a></li>
<li><a class="reference internal" href="#deep-hybrid-bidirectional-lstm-based-rnn">(Deep/Hybrid) Bidirectional LSTM-based RNN</a></li>
</ul>
</li>
<li><a class="reference internal" href="#variants-of-lstm">Variants of LSTM</a><ul>
<li><a class="reference internal" href="#stacked-bottlenecks">Stacked Bottlenecks</a></li>
<li><a class="reference internal" href="#trajectory-modelling">Trajectory modelling</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#models">Models</a><ul>
<li><a class="reference internal" href="#deep-feedforward-recurrent-networks">Deep Feedforward/Recurrent Networks</a></li>
</ul>
</li>
<li><a class="reference internal" href="#layers">Layers</a><ul>
<li><a class="reference internal" href="#recurrent-neural-network-units">Recurrent Neural Network units</a></li>
</ul>
</li>
<li><a class="reference internal" href="#i-o-functions">I/O functions</a><ul>
<li><a class="reference internal" href="#binary-i-o-collections">Binary I/O collections</a></li>
</ul>
</li>
<li><a class="reference internal" href="#utils">Utils</a><ul>
<li><a class="reference internal" href="#data-provider">Data Provider</a></li>
</ul>
</li>
<li><a class="reference internal" href="#front-end">Front-end</a><ul>
<li><a class="reference internal" href="#label-normalisation">Label normalisation</a></li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/index.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2016, Zhizheng.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.3.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.7</a>
      
      |
      <a href="_sources/index.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>